{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jboGx0ZXhgo5"
      },
      "source": [
        "# PRACTICA 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb87yrdchlv1"
      },
      "source": [
        "## 0. Importacion de Modulos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qYjAF2ohrLR",
        "outputId": "45f289f0-8c17-40ec-8e35-3fdf9c30cd6f"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "\n",
        "spanish_stop_words = stopwords.words('spanish')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def txt_to_list(filename):\n",
        "    lines_list = []\n",
        "    with open(filename, 'r') as file:\n",
        "        for line in file:\n",
        "            lines_list.append(line.strip()) # .strip() removes leading/trailing whitespace and newlines\n",
        "    return lines_list\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcR0_01EhtmQ"
      },
      "source": [
        "## 1. Data Wrangling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD8lULBIiBnf"
      },
      "source": [
        "Los datos se obtuvieron de un chat de WhatsApp, posteriormente se exportaron a un TXT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2p_cAADqc_of",
        "outputId": "631c2d8e-ac9d-4f4b-b657-6fbc22523b9c"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "RUTA_ARCHIVO = \"datos/Datos.txt\"\n",
        "\n",
        "PATRON_MENSAJE = re.compile(\n",
        "    r'^(\\d{1,2}/\\d{1,2}/\\d{4}), (\\d{1,2}:\\d{2})\\s?(a\\.?\\s?m\\.?|p\\.?\\s?m\\.?) - (.*?): (.*)'\n",
        ")\n",
        "\n",
        "\n",
        "with open(RUTA_ARCHIVO, encoding=\"utf-8\") as f:\n",
        "    lineas = f.readlines()\n",
        "\n",
        "mensajes = []\n",
        "mensaje_actual = None\n",
        "\n",
        "for linea in lineas:\n",
        "    linea = linea.strip()\n",
        "\n",
        "    match = PATRON_MENSAJE.match(linea)\n",
        "\n",
        "    if match:\n",
        "        # Guardar mensaje anterior\n",
        "        if mensaje_actual:\n",
        "            mensajes.append(mensaje_actual)\n",
        "\n",
        "        fecha, hora, am_pm, autor, texto = match.groups()\n",
        "\n",
        "        mensaje_actual = {\n",
        "            \"Fecha\": fecha,\n",
        "            \"Hora\": hora,\n",
        "            \"AM_PM\": am_pm.lower(),\n",
        "            \"Autor\": autor,\n",
        "            \"Mensaje\": texto\n",
        "        }\n",
        "    else:\n",
        "        # L√≠nea adicional (mensaje multil√≠nea)\n",
        "        if mensaje_actual:\n",
        "            mensaje_actual[\"Mensaje\"] += \" \" + linea\n",
        "\n",
        "# Agregar √∫ltimo mensaje\n",
        "if mensaje_actual:\n",
        "    mensajes.append(mensaje_actual)\n",
        "\n",
        "# =========================\n",
        "# 4. CREACI√ìN DEL DATAFRAME\n",
        "# =========================\n",
        "\n",
        "df_original = pd.DataFrame(mensajes)\n",
        "\n",
        "# =========================\n",
        "# 5. LIMPIEZA B√ÅSICA\n",
        "# =========================\n",
        "\n",
        "# Eliminar mensajes del sistema\n",
        "df_original = df_original[~df_original[\"Autor\"].str.contains(\"mensajes y las llamadas|cre√≥ el grupo|te a√±adi√≥\", case=False, na=False)]\n",
        "\n",
        "# Detectar multimedia\n",
        "df_original[\"Tiene_Multimedia\"] = df_original[\"Mensaje\"].str.contains(\"Multimedia omitido|archivo adjunto\", case=False).astype(int)\n",
        "\n",
        "# Limpiar texto multimedia\n",
        "df_original[\"Mensaje\"] = df_original[\"Mensaje\"].replace(\n",
        "    to_replace=r\"<Multimedia omitido>|archivo adjunto\",\n",
        "    value=\"\",\n",
        "    regex=True\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 6. CONVERSI√ìN DE FECHA Y HORA\n",
        "# =========================\n",
        "\n",
        "def convertir_hora(hora, am_pm):\n",
        "    hora_dt = datetime.strptime(hora, \"%H:%M\")\n",
        "    h = hora_dt.hour\n",
        "\n",
        "    if am_pm.startswith(\"p\") and h != 12:\n",
        "        h += 12\n",
        "    if am_pm.startswith(\"a\") and h == 12:\n",
        "        h = 0\n",
        "\n",
        "    return h + hora_dt.minute / 60\n",
        "\n",
        "\n",
        "df_original[\"Fecha\"] = pd.to_datetime(df_original[\"Fecha\"], format=\"%d/%m/%Y\", errors=\"coerce\")\n",
        "df_original[\"Hora_Num\"] = df_original.apply(lambda x: convertir_hora(x[\"Hora\"], x[\"AM_PM\"]), axis=1)\n",
        "\n",
        "# =========================\n",
        "# 7. FEATURES B√ÅSICAS DE TEXTO\n",
        "# =========================\n",
        "\n",
        "df_original[\"Num_Caracteres\"] = df_original[\"Mensaje\"].str.len()\n",
        "df_original[\"Num_Palabras\"] = df_original[\"Mensaje\"].str.split().str.len()\n",
        "df_original[\"Signo_Pregunta\"] = df_original[\"Mensaje\"].str.contains(r\"\\?\", regex=True).astype(int)\n",
        "\n",
        "# =========================\n",
        "# 8. RESULTADO FINAL\n",
        "# =========================\n",
        "\n",
        "df_original = df_original.reset_index(drop=True)\n",
        "\n",
        "print(df_original.head())\n",
        "print(\"\\nTotal de mensajes procesados:\", len(df_original))\n",
        "\n",
        "df_original.to_csv(\"datos_limpios.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBP6gYptdxB7",
        "outputId": "cf11731d-7d46-4184-d4b8-2d38d04135a9"
      },
      "outputs": [],
      "source": [
        "df_original.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tQTaJp_uT3q"
      },
      "outputs": [],
      "source": [
        "PATRON_EMOJIS = re.compile(\n",
        "    \"[\"\n",
        "    \"\\U0001F600-\\U0001F64F\"  # emoticonos\n",
        "    \"\\U0001F300-\\U0001F5FF\"  # s√≠mbolos y pictogramas\n",
        "    \"\\U0001F680-\\U0001F6FF\"  # transporte y mapas\n",
        "    \"\\U0001F700-\\U0001F77F\"\n",
        "    \"\\U0001F780-\\U0001F7FF\"\n",
        "    \"\\U0001F800-\\U0001F8FF\"\n",
        "    \"\\U0001F900-\\U0001F9FF\"\n",
        "    \"\\U0001FA00-\\U0001FAFF\"\n",
        "    \"\\u2600-\\u26FF\"          # s√≠mbolos varios\n",
        "    \"\\u2700-\\u27BF\"\n",
        "    \"]+\",\n",
        "    flags=re.UNICODE\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6G6priCZuYuC"
      },
      "outputs": [],
      "source": [
        "df_original[\"Mensaje\"] = df_original[\"Mensaje\"].str.replace(\n",
        "    PATRON_EMOJIS,\n",
        "    \"\",\n",
        "    regex=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "KaHduV1Vg4uh",
        "outputId": "fbf77246-d4e5-400c-82b7-f157da4b8311"
      },
      "outputs": [],
      "source": [
        "df = df_original.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "1CK3ji2ehDFH",
        "outputId": "68e0769a-6ebf-44c0-bcd4-3416a43c3ee9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Convertir posibles valores nulos o vac√≠os en la columna Mensaje\n",
        "# Esto limpia espacios en blanco y convierte celdas vac√≠as en NaN\n",
        "df['Mensaje'] = df['Mensaje'].astype(str).str.strip()\n",
        "df['Mensaje'] = df['Mensaje'].replace(['', 'nan', 'None'], np.nan)\n",
        "\n",
        "# 2. Eliminar las filas donde el Mensaje es nulo\n",
        "# Usamos inplace=True para que los cambios se guarden en el mismo DataFrame\n",
        "df.dropna(subset=['Mensaje'], inplace=True)\n",
        "\n",
        "# 3. Opcional: Filtrar filas donde Num_Palabras sea 0\n",
        "# (Como se ve en tus √≠ndices 3 y 4 de la imagen)\n",
        "df = df[df['Num_Palabras'] > 0]\n",
        "\n",
        "# Verificamos cu√°ntas filas quedaron\n",
        "print(f\"Registros despu√©s de la limpieza: {len(df)}\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdNHZCYnDgv6",
        "outputId": "d8b0e270-cb6e-48a6-b545-16aa630f2296"
      },
      "outputs": [],
      "source": [
        "df[\"Mensaje\"].str.extract(r\"\\.([^.]+)$\")[0].unique()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfZK8i-If76f"
      },
      "source": [
        "## 2. Solucion Analƒ±tica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM7fEKfXgFcc"
      },
      "source": [
        "### 2.1 Variable objetivo discreta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Esta variable se utiliza para identificar de manera clara el tema principal de cada mensaje, diferenciando aquellos relacionados con plantas carn√≠voras de los mensajes sobre otros asuntos. Su utilidad principal es medir el grado de enfoque del chat y evaluar si la conversaci√≥n cumple con el prop√≥sito para el cual fue creado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qY_GYkB62ZYZ"
      },
      "outputs": [],
      "source": [
        "palabras_tecnicas = txt_to_list('datos/palabras_tecnicas.txt')\n",
        "print(\"Numero de palabras:\", len(palabras_tecnicas))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QP_hwI2TgBWt"
      },
      "outputs": [],
      "source": [
        "def mensaje_basura(texto):\n",
        "    if pd.isna(texto) or not isinstance(texto, str):\n",
        "        return True\n",
        "\n",
        "    if any(k in texto.lower() for k in ['http', 'www', 'com/', '.com', 'share/']):\n",
        "        return True\n",
        "    if any(ext in texto.lower() for ext in ['.jpg', '.webp', '.png', 'jpg ()', 'webp ()']):\n",
        "        return True\n",
        "    limpio = re.sub(r'[^a-zA-Z\\s]', '', texto).strip()\n",
        "    if len(limpio) < 2:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def normalizar_texto(texto):\n",
        "    if pd.isna(texto): return \"\"\n",
        "\n",
        "\n",
        "    texto = texto.lower()\n",
        "    texto = ''.join(c for c in unicodedata.normalize('NFD', texto) if unicodedata.category(c) != 'Mn')\n",
        "    texto = re.sub(r'\\b(xd|ok|si|no)\\b', '', texto)\n",
        "    texto = re.sub(r'(j|a|x){2,}', '', texto)\n",
        "    texto = re.sub(r'[^a-z\\s]', '', texto)\n",
        "\n",
        "    return ' '.join(texto.split())\n",
        "\n",
        "def calcular_similitud(a, b):\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "def etiquetar_mensaje(mensaje):\n",
        "    texto_limpio = normalizar_texto(mensaje)\n",
        "    palabras_mensaje = texto_limpio.split()\n",
        "\n",
        "    for palabra_m in palabras_mensaje:\n",
        "        for palabra_t in palabras_tecnicas:\n",
        "            if calcular_similitud(palabra_m, palabra_t) >= 0.90:\n",
        "                return 1\n",
        "    return 0\n",
        "\n",
        "# Aplicar al DataFrame\n",
        "print(\"Registros antes de filtrar basura:\", len(df))\n",
        "df[\"Mensaje\"] = df[\"Mensaje\"].apply(normalizar_texto)\n",
        "df = df[df['Mensaje'].apply(lambda x: not mensaje_basura(x))]\n",
        "print(\"Registros despues de filtrar basura:\", len(df))\n",
        "\n",
        "#df[\"Tipo_Mensaje\"] = df[\"Mensaje\"].apply(etiquetar_mensaje)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "embeddings_tecnicos = model.encode(\n",
        "    palabras_tecnicas,\n",
        "    normalize_embeddings=True\n",
        ")\n",
        "\n",
        "def procesar_chunk(textos, threshold=0.75):\n",
        "    embeddings = model.encode(\n",
        "        textos,\n",
        "        normalize_embeddings=True,\n",
        "        batch_size=32,\n",
        "        show_progress_bar=False\n",
        "    )\n",
        "\n",
        "    sims = cosine_similarity(embeddings, embeddings_tecnicos)\n",
        "    max_sims = sims.max(axis=1)\n",
        "\n",
        "    return (max_sims >= threshold).astype(int)\n",
        "\n",
        "\n",
        "def etiquetar_con_hilos(textos, chunk_size=256, max_workers=4):\n",
        "    resultados = [0] * len(textos)\n",
        "\n",
        "    chunks = [\n",
        "        (i, textos[i:i + chunk_size])\n",
        "        for i in range(0, len(textos), chunk_size)\n",
        "    ]\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = {\n",
        "            executor.submit(procesar_chunk, chunk): idx\n",
        "            for idx, chunk in chunks\n",
        "        }\n",
        "\n",
        "        for future in tqdm(\n",
        "            as_completed(futures),\n",
        "            total=len(futures),\n",
        "            desc=\"Procesando chunks\",\n",
        "            unit=\"chunk\"\n",
        "        ):\n",
        "            idx = futures[future]\n",
        "            res = future.result()\n",
        "            resultados[idx:idx + len(res)] = res\n",
        "\n",
        "    return resultados\n",
        "\n",
        "\n",
        "print(\"Registros antes de filtrar basura:\", len(df))\n",
        "\n",
        "df = df[df[\"Mensaje\"].apply(lambda x: not mensaje_basura(x))]\n",
        "df[\"Mensaje\"] = df[\"Mensaje\"].apply(normalizar_texto)\n",
        "\n",
        "print(\"Registros despu√©s de limpiar:\", len(df))\n",
        "\n",
        "df[\"Tipo_Mensaje\"] = etiquetar_con_hilos(\n",
        "    df[\"Mensaje\"].tolist(),\n",
        "    chunk_size=256,\n",
        "    max_workers=4\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv(\"datos/variable_discreta.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgEsJEiA0Ung"
      },
      "source": [
        "Clasificaci√≥n Tipo de Mensaje"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_0dBNF_0bJv"
      },
      "outputs": [],
      "source": [
        "temas = {\n",
        "    \"tecnico\": [\n",
        "        \"planta\", \"sustrato\", \"purpurea\", \"carnivora\", \"hijuelos\", \"cultivo\", \"ra√≠z\"\n",
        "    ],\n",
        "    \"social\": [\n",
        "        \"hola\", \"adios\", \"jaja\", \"üòÇ\", \"ü§£\", \"amigos\", \"gracias\", \"buenas noches\", \"buenos d√≠as\"\n",
        "    ],\n",
        "    \"organizativo\": [\n",
        "        \"reunion\", \"evento\", \"fecha\", \"lugar\", \"hora\", \"proximo\", \"organizar\"\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRz90ghP0fqR"
      },
      "outputs": [],
      "source": [
        "def clasificar_tema(texto):\n",
        "    texto = texto.lower()\n",
        "    for tema, palabras in temas.items():\n",
        "        if any(p in texto for p in palabras):\n",
        "            return tema\n",
        "    return \"otro\"\n",
        "\n",
        "df[\"Tema_Mensaje\"] = df[\"Mensaje\"].apply(clasificar_tema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8nVqbNj0jkh",
        "outputId": "bdd68f58-7234-4e49-ae31-78aae90eae91"
      },
      "outputs": [],
      "source": [
        "print(df[\"Tema_Mensaje\"].value_counts())\n",
        "print(df[[\"Mensaje\", \"Tema_Mensaje\"]].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUEMqlbz8wl5",
        "outputId": "f16b3ebe-0e58-4c5c-d15c-9b1da49d8947"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "FifeLhh49rCy",
        "outputId": "c7db744e-5af7-4618-c1e8-bb611d602c33"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmKt-NZ-clhU",
        "outputId": "e07a5820-cbf8-4962-a4ad-77adbc0ff34e"
      },
      "outputs": [],
      "source": [
        "df['Tipo_Mensaje'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "3RPJOZQGg-Nz",
        "outputId": "223942ac-25f0-4c0c-9840-b0fc30ea9707"
      },
      "outputs": [],
      "source": [
        "df['Tipo_Mensaje'].hist()\n",
        "### Como se puede observar el dataset esta desbalanceado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SknQkGyOi1JD"
      },
      "outputs": [],
      "source": [
        "# Preparacion de Datos\n",
        "X = df[\"Mensaje\"]\n",
        "y = df[\"Tipo_Mensaje\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "vectorizador = TfidfVectorizer(\n",
        "    stop_words=spanish_stop_words,\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=2,\n",
        "    max_df=0.9,\n",
        "    lowercase=True\n",
        ")\n",
        "\n",
        "X_train_tfidf = vectorizador.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizador.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0vCj3StjdZ0"
      },
      "source": [
        "### 2.1.2 Entrenamiento de Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Variable Continua"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Esta variable permite medir qu√© tan t√©cnico o especializado es el contenido de cada mensaje mediante una escala gradual, en lugar de clasificarlo solo como ‚Äúb√°sico‚Äù o ‚Äúavanzado‚Äù. Es √∫til para evaluar la calidad del contenido compartido, identificar mensajes con mayor aporte de conocimiento y entender el nivel general de experiencia presente en el chat. Desde una perspectiva pr√°ctica, ayuda a detectar a los usuarios m√°s experimentados, promover contenido de mayor valor y comprender si la comunidad est√° funcionando como un espacio de aprendizaje y especializaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y855S2uhdzZc"
      },
      "source": [
        "### Regresion Logistica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYUvC6AtuJcS"
      },
      "source": [
        "### Tratanto el Desbalanceo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9X1A48ijhbN",
        "outputId": "f349aaad-7773-4ad0-8168-89f0b1e0e345"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import loguniform\n",
        "\n",
        "log_reg = LogisticRegression(max_iter=1000,solver='saga',  class_weight='balanced')\n",
        "\n",
        "\n",
        "## Ajuste de Hiperparametros\n",
        "param_dist = {\n",
        "    'C': loguniform(1e-4, 1e2),\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "    'l1_ratio': [0, 0.25, 0.5, 0.75, 1]\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=log_reg,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=30,\n",
        "    scoring='f1',\n",
        "    cv=3,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "random_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "## Mejor Modelo\n",
        "modelo = random_search.best_estimator_\n",
        "\n",
        "print(\"Mejores hiperpar√°metros:\")\n",
        "print(random_search.best_params_)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ho5tqSQCjm3b",
        "outputId": "c933216b-3b23-4d39-ed56-8175b3f4985b"
      },
      "outputs": [],
      "source": [
        "# Evaluacion\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "y_pred = modelo.predict(X_test_tfidf)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18oYRNV6vjRV"
      },
      "source": [
        "### Sin Tratar el Desbalanceo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdySCSErvmfl",
        "outputId": "3e55eee8-df7c-418d-fede-2a3fff497505"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Sin tratar desbalance\n",
        "# -----------------------------\n",
        "log_reg = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    solver='saga'\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "param_dist = {\n",
        "    'C': loguniform(1e-4, 1e2),\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "    'l1_ratio': [0, 0.25, 0.5, 0.75, 1]\n",
        "}\n",
        "\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=log_reg,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=30,\n",
        "    scoring='f1',\n",
        "    cv=3,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "random_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "\n",
        "modelo_sin_desbalanceo = random_search.best_estimator_\n",
        "\n",
        "print(\"Mejores hiperpar√°metros:\")\n",
        "print(random_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HthTUXWv8UV",
        "outputId": "973867fe-17f9-4dbb-af31-f70209da91bb"
      },
      "outputs": [],
      "source": [
        "# Evaluacion\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "y_pred = modelo_sin_desbalanceo .predict(X_test_tfidf)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn3ukyLpxEQh"
      },
      "source": [
        "### Palabras Asociadas a mensajes Tecnicos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGNNLbpYjqNg",
        "outputId": "bce2f9b3-67a6-4781-8f1a-61840e8c4a89"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "feature_names = vectorizador.get_feature_names_out()\n",
        "coeficientes = modelo.coef_[0]\n",
        "\n",
        "top_palabras = sorted(\n",
        "    zip(feature_names, coeficientes),\n",
        "    key=lambda x: x[1],\n",
        "    reverse=True\n",
        ")[:100]\n",
        "\n",
        "print(\"Palabras m√°s asociadas a mensajes t√©cnicos:\")\n",
        "for palabra, peso in top_palabras:\n",
        "    print(palabra, round(peso, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "RDoXKE0N7Xr_",
        "outputId": "9ad01cc7-1dfb-4d53-f3b8-c1fc92e6c51f"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Matriz de confusi√≥n normalizada por fila\n",
        "cm = confusion_matrix(y_test, y_pred, labels=modelo.classes_)\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Visualizaci√≥n\n",
        "sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap=\"Greens\",\n",
        "            xticklabels=modelo.classes_, yticklabels=modelo.classes_)\n",
        "plt.xlabel(\"Predicci√≥n\")\n",
        "plt.ylabel(\"Real\")\n",
        "plt.title(\"Matriz de Confusi√≥n Normalizada (%)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdgWzMjhePHl"
      },
      "source": [
        "### Maquinas de Soporte Vectorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vpUwNfQeRYX"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "svm = LinearSVC(\n",
        "    class_weight='balanced',\n",
        "    max_iter=2000\n",
        ")\n",
        "\n",
        "# Ajuste de Hiperparametro\n",
        "param_dist = {\n",
        "    'C': loguniform(1e-4, 1e2),\n",
        "    'loss': ['hinge', 'squared_hinge']\n",
        "}\n",
        "\n",
        "random_search_svm = RandomizedSearchCV(\n",
        "    estimator=svm,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=40,\n",
        "    scoring='f1',\n",
        "    cv=5,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "random_search_svm.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Mejor modelo\n",
        "modelo_svm = random_search_svm.best_estimator_\n",
        "\n",
        "print(\"Mejores hiperpar√°metros (SVM):\")\n",
        "print(random_search_svm.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8C-eo_Ienpc"
      },
      "outputs": [],
      "source": [
        "### Evaluacion\n",
        "y_pred = modelo.predict(X_test_tfidf)\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmctO94RoN0X"
      },
      "source": [
        "## 2.2 Modelo Variable Objetivo Continua"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHzR_CMl2ACg"
      },
      "outputs": [],
      "source": [
        "ANCHOR_EXPERT = [\n",
        "    \"Nepenthes villosa es una especie de alta monta√±a que requiere noches fr√≠as, alta humedad y excelente oxigenaci√≥n radicular\",\n",
        "    \"El peristoma de Nepenthes funciona como una superficie resbaladiza cuando est√° h√∫medo, facilitando la captura de presas\",\n",
        "    \"Las Nepenthes de tierras altas presentan metabolismo adaptado a temperaturas nocturnas bajas\",\n",
        "    \"El sustrato mineral con buen drenaje reduce el riesgo de pudrici√≥n radicular en Nepenthes\",\n",
        "    \"Sarracenia presenta rizomas subterr√°neos y requiere un periodo de dormancia invernal para un crecimiento saludable\",\n",
        "    \"Las Sarracenia obtienen nutrientes principalmente de insectos atrapados en sus ascidios\",\n",
        "    \"La falta de dormancia puede debilitar progresivamente a las Sarracenia\",\n",
        "    \"Drosera captura presas mediante muc√≠lago producido por tricomas glandulares\",\n",
        "    \"La digesti√≥n en Drosera ocurre mediante enzimas secretadas sobre la presa atrapada\",\n",
        "    \"Pinguicula presenta hojas con gl√°ndulas pegajosas que permiten la captura de peque√±os insectos\",\n",
        "    \"Algunas especies de Pinguicula desarrollan hojas no carn√≠voras durante la estaci√≥n seca\",\n",
        "    \"Los h√≠bridos interespec√≠ficos de Nepenthes pueden mostrar dominancia fenot√≠pica del peristoma\",\n",
        "    \"La identificaci√≥n taxon√≥mica de Nepenthes se basa en caracter√≠sticas como el peristoma, la tapa y el indumento\",\n",
        "    \"Existen complejos de especies en Nepenthes que dificultan su clasificaci√≥n taxon√≥mica\",\n",
        "    \"El exceso de humedad estancada favorece infecciones f√∫ngicas en plantas carn√≠voras\",\n",
        "    \"La ventilaci√≥n constante es clave para evitar pat√≥genos en cultivos de Nepenthes\",\n",
        "    \"El uso de agua con baja conductividad es fundamental para el cultivo de plantas carn√≠voras\",\n",
        "    \"Muchas plantas carn√≠voras habitan suelos pobres en nutrientes, lo que explica su estrategia carn√≠vora\",\n",
        "    \"La captura de insectos permite a las plantas carn√≠voras suplementar nitr√≥geno y f√≥sforo\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCIPG2e52Pzr"
      },
      "outputs": [],
      "source": [
        "ANCHOR_CASUAL = [\n",
        "    \"qu√© bonita planta\",\n",
        "    \"me gusta mucho esa planta\",\n",
        "    \"se ve bien chula\",\n",
        "    \"est√° preciosa\",\n",
        "    \"me encantan las plantas\",\n",
        "    \"esa planta est√° rara\",\n",
        "    \"nunca hab√≠a visto una as√≠\",\n",
        "    \"yo quiero una\",\n",
        "    \"d√≥nde la compraste\",\n",
        "    \"estoy imprimiendo una pieza en 3D\",\n",
        "    \"mi impresora 3D fall√≥ otra vez\",\n",
        "    \"ya termin√© de imprimir el soporte\",\n",
        "    \"estoy dise√±ando una pieza en fusion 360\",\n",
        "    \"esa pieza qued√≥ perfecta en PLA\",\n",
        "    \"voy a soldar unos cables\",\n",
        "    \"el ventilador ya no prende\",\n",
        "    \"tengo que cambiar el relay\",\n",
        "    \"el controlador dej√≥ de funcionar\",\n",
        "    \"buenos d√≠as\",\n",
        "    \"jajaja\",\n",
        "    \"no inventes\",\n",
        "    \"qu√© onda\",\n",
        "    \"gracias\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f25GndF2cjQ"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "expert_vec = embedder.encode(\n",
        "    ANCHOR_EXPERT,\n",
        "    normalize_embeddings=True\n",
        ").mean(axis=0)\n",
        "\n",
        "casual_vec = embedder.encode(\n",
        "    ANCHOR_CASUAL,\n",
        "    normalize_embeddings=True\n",
        ").mean(axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36OPZBAp2uvh"
      },
      "outputs": [],
      "source": [
        "from numpy.linalg import norm\n",
        "\n",
        "def specialization_score_embedding(text: str) -> float:\n",
        "    if not isinstance(text, str) or text.strip() == \"\":\n",
        "        return 0.0\n",
        "\n",
        "    vec = embedder.encode(text, normalize_embeddings=True)\n",
        "\n",
        "    sim_expert = np.dot(vec, expert_vec)\n",
        "    sim_casual = np.dot(vec, casual_vec)\n",
        "\n",
        "    score = (sim_expert - sim_casual + 1) / 2\n",
        "    return round(np.clip(score, 0, 1), 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cb5FiwqP2uth"
      },
      "outputs": [],
      "source": [
        "df[\"especializacion\"] = df[\"Mensaje\"].apply(specialization_score_embedding)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkhfCKmK5RWp"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQU8yNk73mbg"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"datos_etiquetados1.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrCV2JbrMJmu"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Cargar modelo\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Vectores promedio de referencia\n",
        "expert_vec = embedder.encode(ANCHOR_EXPERT, normalize_embeddings=True).mean(axis=0)\n",
        "casual_vec = embedder.encode(ANCHOR_CASUAL, normalize_embeddings=True).mean(axis=0)\n",
        "\n",
        "# Procesar todos los mensajes en lote\n",
        "mensajes = df[\"Mensaje\"].tolist()\n",
        "embeddings = embedder.encode(mensajes, batch_size=32, show_progress_bar=True)\n",
        "\n",
        "# Calcular puntajes continuos en lote\n",
        "sim_expert = np.dot(embeddings, expert_vec)\n",
        "sim_casual = np.dot(embeddings, casual_vec)\n",
        "scores = (sim_expert - sim_casual + 1) / 2\n",
        "\n",
        "# Guardar en el DataFrame con dos decimales\n",
        "df[\"especializacion\"] = np.clip(scores, 0, 1).round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IefJch43MMcv"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dp3ZGwqoMSLJ"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"Datos_Etiquetados.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
